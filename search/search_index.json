{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>This is the project page of flooder, an easy-to-use Python package for constructing a lightweight simplicial complex (i.e., the FLood complex) on top of (low-dimensional) Euclidean point cloud data and subsequent persistent homology<sup>1</sup> (PH) computation. Algorithmically, flooder is designed to take full advantage of state-of-the-art GPU computing frameworks (via PyTorch) to enable computation of a filtered simplicial complex on millions of points in seconds. Based on the Flood complex, we use the awesome gudhi library for PH computation.</p>"},{"location":"#illustration","title":"Illustration","text":"<p>Below is an illustrative animation on how the (filtered) Flood complex is built (on a noisy sample of a figure-eight shape in 2D), starting from a collection of points and a Delaunay triangulation of a small subset of landmarks (in yellow). In short, a simplex of the Delaunay triangulation is added to the Flood complex at time \\(t\\geq 0\\), if it is fully covered by balls of radius \\(t\\), centered at all data points.</p>"},{"location":"#runtime-teaser","title":"Runtime teaser","text":"<p>Below you find a runtime comparison of computing zero-, one-, and two-dimensional Flood and Alpha PH on 1M points in 3D (using a NVIDIA H100 NVL GPU). The data used in this comparison is a synthetic (3D) swiss cheese created by uniformly drawing points in the unit cube and introducing a certain number of voids. Note that on point clouds of this size (in fact, even at much lower scale), computing Vietoris-Rips PH is intractable (beyond 0-dimensional features). The table lists runtime in seconds (for creating the filtered Flood and Alpha complexes and subsequent PH computation).</p> Complex  Runtime (in s) Alpha complex (via <code>gudhi.AlphaComplex</code>) 141.8 \\(\\pm\\) 1.5 Flood complex (via flooder) 1.4 \\(\\pm\\) 0.3 <p>Please see our <code>examples</code> folder in the flooder GitHub repository to run your own runtime comparison.</p> <p>Caution</p> <p>flooder is still under active development and it's usage API might change significantly over the next months.</p> <ol> <li> <p>Edelsbrunner, Letscher &amp; Zomorodian. Topological Persistence and Simplification. Discrete Comput Geom 28, 511\u2013533 (2002). DOI \u21a9</p> </li> </ol>"},{"location":"contributing/","title":"Contributing","text":"<p>Here's a step-by-step example of how to contribute to <code>flooder</code>.</p>"},{"location":"contributing/#fork-the-repository","title":"Fork the repository","text":"<p>On GitHub, click \"Fork\" to create your personal copy of the repository in your GitHub account, then    clone your fork. In the following, lets call the fork <code>flooder-devel</code>.</p> <pre><code>git clone https://github.com/rkwitt/flooder-devel.git\ncd flooder-devel\n</code></pre>"},{"location":"contributing/#add-upstream-remote","title":"Add upstream remote","text":"<pre><code>git remote add upstream https://github.com/plus-rkwitt/flooder.git   \n</code></pre>"},{"location":"contributing/#sync-your-local-main","title":"Sync your local main","text":"<p>Surely, if you just forked, everything will be in sync (but just to be sure :)</p> <pre><code>git checkout main\ngit fetch upstream\ngit rebase upstream/main\ngit push origin main\n</code></pre>"},{"location":"contributing/#create-a-feature-branch","title":"Create a feature branch","text":"<p>Next, we create a feature branch which will contain our adjustments/enhancements/etc.</p> <pre><code>git checkout -b fix-typos\n</code></pre>"},{"location":"contributing/#make-changes-and-commit","title":"Make changes and commit","text":"<p>Once you are done with your changes, commit.</p> <pre><code>git commit -a -m \"ENH: Fixed some typos.\"\n</code></pre> <p>What if <code>upstream/main</code> divereged in the meantime (e.g., a PR     got merged or so)?</p> <pre><code>git fetch upstream\ngit rebase upstream/main\n</code></pre> <p>Fix files in case of conflicts, then add them and continue the rebase.</p> <pre><code>git add &lt;file&gt;\ngit rebase --continue\n</code></pre>"},{"location":"contributing/#push-your-branch-to-your-fork","title":"Push your branch to your fork","text":"<pre><code>git push --force-with-lease origin fix-typos\n</code></pre>"},{"location":"contributing/#open-a-pr-on-github","title":"Open a PR on GitHub","text":"<p>Finally, create a pull request on GitHub.</p> <ul> <li>Navigate to your fork on GitHub.</li> <li>Click \"Compare &amp; pull request\".</li> <li>Submit the pull request to the upstream repository.</li> </ul> <p>PR's will be reviewed by the main developers of <code>flooder</code>, possibly commented, and then merged in case of no conflicts or concerns.</p>"},{"location":"contributing/#cleanup","title":"Cleanup","text":"<p>Once the PR is merged, we cleanup the branch in the forked repo.</p> <pre><code>git branch -d fix-typos\ngit push origin --delete fix-typos\n</code></pre>"},{"location":"examples/","title":"Examples","text":""},{"location":"examples/#flood-ph-of-noisy-torus-points","title":"Flood PH of noisy torus points","text":"<p>In the following example, we sample <code>n_pts=1000000</code> points from a noisy torus in 3D, then construct the Flood complex on top of <code>n_lms=1000</code> landmarks (see Illustration ) and compute persistent homology (up to dimension 3) using <code>gudhi</code> based on the constructed filtered Flood simplicial complex.</p> <pre><code>from flooder import (\n    generate_noisy_torus_points_3d, \n    flood_complex, \n    generate_landmarks)\n\nDEVICE = \"cuda\"\nn_pts = 1_000_000  # Number of points to sample from torus\nn_lms = 1_000      # Number of landmarks for Flood complex\n\npts = generate_noisy_torus_points_3d(n_pts).to(DEVICE)\nlms = generate_landmarks(pts, n_lms)\n\nstree = flood_complex(pts, lms, return_simplex_tree=True)\nstree.compute_persistence()\nph_diags = [stree.persistence_intervals_in_dimension(i) for i in range(3)]\n</code></pre> <p>Importantly, one can either call <code>flood_complex</code> with the already pre-selected (here via FPS) landmarks, or one can just specify the number of desired landmarks, e.g., via</p> <pre><code>stree = flood_complex(pts, n_lms, return_simplex_tree=True)\n</code></pre> <p>in which case FPS is called internally.</p>"},{"location":"examples/#flooder-cli","title":"Flooder CLI","text":"<p>If you installed flooder via <code>pip install flooder</code>, you not only have the API  available but also a command-line interface (CLI), which you can execute as  follows: for demonstration, we will download a PLY file of the Lucy angel (&gt;14M points, available  from the Stanford 3D Scanning Repository) </p> <pre><code>wget https://graphics.stanford.edu/data/3Dscanrep/lucy.tar.gz\ntar xvfz lucy.tar.gz\n</code></pre> <p>and first convert it (using the Open3D library; install via <code>pip install open3d</code>) to a  <code>(N,3)</code> numpy array:</p> <pre><code>import numpy as np\nimport open3d as o3d\nX_ply = o3d.io.read_point_cloud(\"lucy.ply\")\nX_np = np.asarray(X_ply.points, dtype=np.float32)\nnp.save(\"lucy.npy\", X_np)\n</code></pre> <p>Finally, we execute the Flooder CLI, using 5k landmarks, 30 points per edge and a batch size of 64:</p> <pre><code>flooder \\\n  --input-file lucy.npy \\\n  --output-file lucy-diagrams.pkl \\\n  --num-landmarks 5000 \\\n  --batch-size 64 \\\n  --max-dimension 3 \\\n  --points-per-edge 30 \\\n   --device cuda:0 \\\n  --cuda-events \\\n  --stats-json lucy.json\n</code></pre>"},{"location":"install/","title":"Installation","text":""},{"location":"install/#environment-setup","title":"Environment setup","text":"<p>We recommend installing flooder in a clean new Anaconda environment as described below:</p> <pre><code>conda create -n flooder-env python=3.9 -y\nconda activate flooder-env\nconda install pip git -y\n</code></pre>"},{"location":"install/#via-pip-recommended","title":"Via Pip (recommended)","text":"<p>flooder is available on PyPi (here) and can be installed via:</p> <pre><code>pip install flooder\n</code></pre> <p>To test the command-line interface (CLI) use </p> <pre><code>flooder --help \n</code></pre>"},{"location":"install/#development-installation","title":"Development installation","text":"<p>In case you want to contribute to flooder, clone the GitHub repo and run</p> <pre><code>git clone https://github.com/plus-rkwitt/flooder\ncd flooder\npip install -e .\n</code></pre> <p>In case you do not want to install anything, you can also execute examples or tests from within the checked-out folder by specifying <code>PYTHONPATH</code>as</p> <pre><code>git clone https://github.com/plus-rkwitt/flooder\ncd flooder\nPYTHONPATH=. python examples/example_01_cheese_3d.py\n</code></pre>"},{"location":"install/#gpu-requirements","title":"GPU requirements","text":"<p>Our implementation relies heavily on custom Triton kernels (although we support CPU computation as well) for maximum performance. According to the official Triton compatibility page, you need a NVIDIA GPU with compute capabilty (check here) of at least 7.5 (e.g., GTX 3080, etc.).</p>"},{"location":"visualizations/","title":"Visualizations","text":"<p>We visualize the Flood complex at different radii on selected point clouds. All input point clouds consist of 1 million points and we always build the complex using 1000 landmarks.</p>"},{"location":"visualizations/#virus-rv-a89","title":"Virus (RV-A89)","text":"<p>A point cloud extracted from cryo-electron microscopy map of a rhinovirus </p> \u25c0 Back Next \u25b6"},{"location":"visualizations/#coral","title":"Coral","text":"<p>An Acropora cervicornis coral from the Smithsonian 3D Digitization program. </p> \u25c0 Back Next \u25b6"},{"location":"visualizations/#lockwasher","title":"Lockwasher","text":"<p>A lockwasher from the mcb dataset.</p> \u25c0 Back Next \u25b6"},{"location":"reference/core/","title":"Core","text":""},{"location":"reference/core/#flooder.core","title":"flooder.core","text":"<p>Implementation of flooder core functionality.</p> <p>Copyright (c) 2025 Paolo Pellizzoni, Florian Graf, Martin Uray, Stefan Huber and Roland Kwitt SPDX-License-Identifier: MIT</p>"},{"location":"reference/core/#flooder.core.flood_complex","title":"flood_complex","text":"<pre><code>flood_complex(\n    points: Tensor,\n    landmarks: Union[int, Tensor],\n    max_dimension: Union[None, int] = None,\n    points_per_edge: Union[None, int] = 30,\n    num_rand: int = None,\n    batch_size: Union[None, int] = 64,\n    use_triton: Optional[bool] = None,\n    return_simplex_tree: bool = False,\n    fps_h: Union[None, int] = None,\n    start_idx: Union[int, None] = 0,\n) -&gt; Union[dict, gudhi.SimplexTree]\n</code></pre> <p>Constructs a Flood complex from a set of witness points and landmarks.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>Tensor</code> <p>A (N, d) tensor containing witness points used as sources in the flood process.</p> required <code>landmarks</code> <code>Union[int, Tensor]</code> <p>Either an integer indicating the number of landmarks to randomly sample from <code>points</code>, or a tensor of shape (N_l, d) specifying explicit landmark coordinates.</p> required <code>max_dimension</code> <code>Union[None, int]</code> <p>The top dimension of the simplices to construct. Defaults to None resulting in the dimension of the ambient space.</p> <code>None</code> <code>points_per_edge</code> <code>Union[None, int]</code> <p>Specifies resolution on simplices used for computing filtration values. Tradeoff in accuracy vs. speed. Defaults to 30.</p> <code>30</code> <code>num_rand</code> <code>Union[None, int]</code> <p>If specified, filtration values are computed from a fixed number of random points per simplex. Defaults to None.</p> <code>None</code> <code>batch_size</code> <code>int</code> <p>Number of simplices to process per batch. Defaults to 32.</p> <code>64</code> <code>use_triton</code> <code>bool</code> <p>If True, Triton kernel is used. Defaults to None (in which case we use Triton if available).</p> <code>None</code> <code>fps_h</code> <code>Union[None, int]</code> <p>h parameter (depth of kdtree) that is used for farthest point sampling to select the landmarks. If None, then h is selected based on the size of the point cloud. Defaults to None.</p> <code>None</code> <code>return_simplex_tree</code> <code>bool</code> <p>I true, a gudhi.SimplexTree is returned, else a dictionary. Defaults to False</p> <code>False</code> <code>start_idx</code> <code>int | None</code> <p>If provided, FPS starts from this index in the point cloud. If not, the start index will be randomly picked from the point cloud. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>Union[dict, SimplexTree]</code> <p>Union[dict, gudhi.SimplexTree] Depending on the return_simplex_tree argument either a gudhi.SimplexTree or a dictionary is returned, mapping simplices to their estimated covering radii (i.e., filtration value). Each key is a tuple of landmark indices (e.g., (i, j) for an edge), and each value is a float radius.</p>"},{"location":"reference/core/#flooder.core.generate_grid","title":"generate_grid","text":"<pre><code>generate_grid(\n    n: int, dim: int, device: device, dtype: dtype\n) -&gt; Tuple[\n    torch.Tensor, List[torch.Tensor], List[torch.Tensor]\n]\n</code></pre> <p>Generates a grid of points on the unit simplex based on the number of points per edge.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of points per edge.</p> required <code>dim</code> <code>int</code> <p>Dimension of the simplex.</p> required <code>device</code> <code>device</code> <p>Device to create the tensors on.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[Tensor, List[Tensor], List[Tensor]]</code> <p>grid (torch.Tensor):     Tensor of shape (C, dim + 1), containing the grid points (coordinate weights). vertex_ids (list of torch.Tensor):     A list of tensors, each containing the vertex indices for each face. face_ids (list of torch.Tensor):     A list of tensors, each containing the face indices for each face.</p>"},{"location":"reference/core/#flooder.core.generate_landmarks","title":"generate_landmarks","text":"<pre><code>generate_landmarks(\n    points: Tensor,\n    n_lms: int,\n    fps_h: Union[None, int] = None,\n    start_idx: Union[int, None] = None,\n) -&gt; torch.Tensor\n</code></pre> <p>Selects landmarks using Farthest-Point Sampling (bucket FPS).</p> <p>This method implements a variant of Farthest-Point Sampling from here.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>Tensor</code> <p>A (P, d) tensor representing a point cloud. The tensor may reside on any device (CPU or GPU) and be of any floating-point dtype.</p> required <code>n_lms</code> <code>int</code> <p>The number of landmarks to sample (must be &lt;= P and &gt; 0).</p> required <code>fps_h</code> <code>Union[None, int]</code> <p>h parameter (depth of kdtree) that is used for farthest point sampling to select the landmarks. If None, then h is selected based on the size of the point cloud. Defaults to None.</p> <code>None</code> <code>start_idx</code> <code>int | None</code> <p>If provided, the sampling starts from this index in the point cloud. If not, the start index will be randomly picked from the point cloud.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A (n_l, d) tensor containing a subset of the input <code>points</code>, representing the sampled landmarks. Returned tensor is on the same device and has the same dtype as the input.</p>"},{"location":"reference/core/#flooder.core.generate_uniform_weights","title":"generate_uniform_weights","text":"<pre><code>generate_uniform_weights(num_rand, dim, device, dtype)\n</code></pre> <p>Generates num_rand points from a uniform distribution on the unit simplex.</p> <p>Parameters:</p> Name Type Description Default <code>num_rand</code> <code>int</code> <p>Number of random points to generate.</p> required <code>dim</code> <code>int</code> <p>Dimension of the simplex.</p> required <code>device</code> <code>device</code> <p>Device to create the tensor on.</p> required <p>Returns:     torch.Tensor:         A tensor of shape [num_rand, dim + 1] containing the random points         (coordinate weights).</p>"},{"location":"reference/datasets/","title":"Datasets","text":""},{"location":"reference/datasets/#flooder.datasets.datasets","title":"flooder.datasets.datasets","text":"<p>Implementation of datasets used in the original Flooder paper.</p> <p>Copyright (c) 2025 Paolo Pellizzoni, Florian Graf, Martin Uray, Stefan Huber and Roland Kwitt SPDX-License-Identifier: MIT</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.BaseDataset","title":"BaseDataset","text":"<pre><code>BaseDataset(\n    root: str,\n    fixed_transform: Callable[[FlooderData], FlooderData]\n    | None = None,\n    transform: Callable[[FlooderData], FlooderData]\n    | None = None,\n)\n</code></pre> <p>               Bases: <code>Dataset</code></p> <p>Base class for Flooder datasets with download/process/load lifecycle.</p> <p>This class provides a dataset API inspired by <code>torch_geometric.data.Dataset</code>, including:   - a standard directory layout (<code>root/raw</code> and <code>root/processed</code>);   - a lifecycle executed at construction time: download, process, load;   - integer indexing to return items, and advanced indexing to return     a subset \"view\" of the dataset;   - optional per-sample transformations.</p> <p>Subclasses must implement the abstract properties/methods that specify file requirements and dataset-specific loading logic.</p> <p>Attributes:</p> Name Type Description <code>root</code> <code>str</code> <p>Root directory containing the dataset folders.</p> <code>fixed_transform</code> <code>Callable[[FlooderData], FlooderData] | None</code> <p>Optional transform applied once to each item during <code>_load()</code> (i.e., at dataset load time, before storing in memory).</p> <code>transform</code> <code>Callable[[FlooderData], FlooderData] | None</code> <p>Optional transform applied on-the-fly in <code>__getitem__</code> for individual samples.</p> <code>_indices</code> <code>Sequence[int] | None</code> <p>If not None, defines a subset view over the underlying dataset indices.</p> Notes <ul> <li>The constructor triggers <code>_download()</code>, <code>_process()</code>, and <code>_load()</code>.   This means instantiation may perform I/O and compute.</li> <li>Advanced indexing (<code>slice</code>, sequences, boolean masks) returns a   shallow-copied dataset object sharing the same underlying storage   (whatever the subclass uses), but with <code>_indices</code> set.</li> </ul> <p>Initialize a dataset and execute the download/process/load lifecycle.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>Root directory where raw and processed data are stored.</p> required <code>fixed_transform</code> <code>Callable[[FlooderData], FlooderData] | None</code> <p>Optional transform applied once to each item during <code>_load()</code>.</p> <code>None</code> <code>transform</code> <code>Callable[[FlooderData], FlooderData] | None</code> <p>Optional transform applied on-the-fly in <code>__getitem__</code>.</p> <code>None</code> Notes <p>Instantiation may perform I/O and compute by calling <code>_download()</code>, <code>_process()</code>, and <code>_load()</code>.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.BaseDataset.processed_dir","title":"processed_dir  <code>property</code>","text":"<pre><code>processed_dir: str\n</code></pre> <p>Directory containing processed files.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to <code>&lt;root&gt;/processed</code>.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.BaseDataset.processed_file_names","title":"processed_file_names  <code>property</code>","text":"<pre><code>processed_file_names: Union[str, List[str], Tuple[str, ...]]\n</code></pre> <p>Required processed files to consider the dataset processed.</p> <p>Subclasses should return the file name(s) expected to exist in <code>processed_dir</code>. If all such files exist, <code>_process()</code> is skipped.</p> <p>Returns:</p> Type Description <code>Union[str, List[str], Tuple[str, ...]]</code> <p>Union[str, list[str], tuple[str, ...]]: File name(s) expected</p> <code>Union[str, List[str], Tuple[str, ...]]</code> <p>inside <code>self.processed_dir</code>.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not implemented by a subclass.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.BaseDataset.processed_paths","title":"processed_paths  <code>property</code>","text":"<pre><code>processed_paths: List[str]\n</code></pre> <p>Absolute paths to required processed files.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>list[str]: List of absolute paths for <code>processed_file_names</code> under</p> <code>List[str]</code> <p><code>processed_dir</code>.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.BaseDataset.raw_dir","title":"raw_dir  <code>property</code>","text":"<pre><code>raw_dir: str\n</code></pre> <p>Directory containing raw downloaded files.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to <code>&lt;root&gt;/raw</code>.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.BaseDataset.raw_file_names","title":"raw_file_names  <code>property</code>","text":"<pre><code>raw_file_names: Union[str, List[str], Tuple[str, ...]]\n</code></pre> <p>Required raw files to consider the dataset downloaded.</p> <p>Subclasses should return the file name(s) expected to exist in <code>raw_dir</code>. If all such files exist, <code>_download()</code> is skipped.</p> <p>Returns:</p> Type Description <code>Union[str, List[str], Tuple[str, ...]]</code> <p>Union[str, list[str], tuple[str, ...]]: File name(s) expected</p> <code>Union[str, List[str], Tuple[str, ...]]</code> <p>inside <code>self.raw_dir</code>.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not implemented by a subclass.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.BaseDataset.raw_paths","title":"raw_paths  <code>property</code>","text":"<pre><code>raw_paths: List[str]\n</code></pre> <p>Absolute paths to required raw files.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>list[str]: List of absolute paths for <code>raw_file_names</code> under</p> <code>List[str]</code> <p><code>raw_dir</code>.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.BaseDataset.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(\n    idx: Union[int, integer, IndexType],\n) -&gt; \"FlooderData | BaseDataset\"\n</code></pre> <p>Get an item or a subset of the dataset.</p> <p>Behavior depends on the type of <code>idx</code>:</p> <ul> <li>If <code>idx</code> is an integer (Python <code>int</code>, <code>np.integer</code>, 0-dim <code>Tensor</code>,   or scalar <code>np.ndarray</code>), returns a single <code>FlooderData</code> object   corresponding to the view index <code>idx</code>.</li> <li>Otherwise, returns a subset view of the dataset created via   <code>index_select(idx)</code>.</li> </ul> <p>If <code>transform</code> is set, it is applied on-the-fly to single-item access.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int | integer | slice | Tensor | ndarray | Sequence</code> <p>Index or indices selecting items.</p> required <p>Returns:</p> Type Description <code>'FlooderData | BaseDataset'</code> <p>FlooderData | BaseDataset: A single data object if <code>idx</code> is scalar-like, otherwise a <code>BaseDataset</code> subset view.</p> <p>Raises:</p> Type Description <code>IndexError</code> <p>If <code>idx</code> type is unsupported (delegated to <code>index_select</code>).</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.BaseDataset.__iter__","title":"__iter__","text":"<pre><code>__iter__() -&gt; Iterator[FlooderData]\n</code></pre> <p>Iterate over items in the current dataset view.</p> <p>Yields:</p> Name Type Description <code>FlooderData</code> <code>FlooderData</code> <p>Items in order from <code>0</code> to <code>len(self) - 1</code>, with</p> <code>FlooderData</code> <p><code>transform</code> applied if configured.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.BaseDataset.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Return the number of examples in the current dataset view.</p> <p>For a full dataset this equals <code>self.len()</code>. For a subset view this equals <code>len(self._indices)</code>.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of examples exposed by this dataset instance.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.BaseDataset.download","title":"download","text":"<pre><code>download() -&gt; None\n</code></pre> <p>Download the dataset into <code>raw_dir</code>.</p> <p>Subclasses must implement the dataset-specific download logic. This method is called by <code>_download()</code> only if the required files in <code>raw_paths</code> are not present.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not implemented by a subclass.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.BaseDataset.get","title":"get","text":"<pre><code>get(idx: int) -&gt; FlooderData\n</code></pre> <p>Return the data object at a given global index.</p> <p><code>idx</code> refers to the underlying dataset index, not the subset-view index. The subset mapping is handled by <code>__getitem__</code> via <code>indices()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Global index into the underlying dataset storage.</p> required <p>Returns:</p> Name Type Description <code>FlooderData</code> <code>FlooderData</code> <p>The data object at the given index.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not implemented by a subclass.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.BaseDataset.index_select","title":"index_select","text":"<pre><code>index_select(idx: IndexType) -&gt; 'BaseDataset'\n</code></pre> <p>Create a subset view of the dataset from specified indices.</p> <p>Supported index types: - <code>slice</code>: includes support for float boundaries, e.g. <code>dataset[:0.9]</code>,   interpreted as a fraction of the current view length. - <code>torch.Tensor</code> of dtype <code>long</code>: treated as integer indices. - <code>torch.Tensor</code> of dtype <code>bool</code>: treated as a boolean mask. - <code>np.ndarray</code> of dtype <code>int64</code>: treated as integer indices. - <code>np.ndarray</code> of dtype <code>bool</code>: treated as a boolean mask. - <code>Sequence</code> (excluding <code>str</code>): treated as a list of integer indices.</p> <p>The returned dataset is a shallow copy of <code>self</code> with <code>_indices</code> set to map view indices to global indices.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>slice | Sequence[int] | Tensor | ndarray</code> <p>Indices specifying the subset.</p> required <p>Returns:</p> Name Type Description <code>BaseDataset</code> <code>'BaseDataset'</code> <p>A subset view of the dataset.</p> <p>Raises:</p> Type Description <code>IndexError</code> <p>If <code>idx</code> is not one of the supported types or has an unsupported dtype.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.BaseDataset.indices","title":"indices","text":"<pre><code>indices() -&gt; Sequence\n</code></pre> <p>Return the active index mapping for this dataset view.</p> <p>For a full dataset (no subset), this is <code>range(self.len())</code>. For a subset view created via <code>index_select</code>, this is the stored <code>_indices</code> sequence.</p> <p>Returns:</p> Type Description <code>Sequence</code> <p>Sequence[int]: Index mapping from view indices to global indices.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.BaseDataset.len","title":"len","text":"<pre><code>len() -&gt; int\n</code></pre> <p>Return the number of items in the full dataset.</p> <p>This method is analogous to <code>torch_geometric.data.Dataset.len()</code>. It should return the total number of items in the underlying dataset, not the size of a subset view created via <code>index_select</code>.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Total number of data objects stored by the dataset.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not implemented by a subclass.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.BaseDataset.process","title":"process","text":"<pre><code>process() -&gt; None\n</code></pre> <p>Process raw files into <code>processed_dir</code>.</p> <p>Subclasses must implement the dataset-specific processing logic. This method is called by <code>_process()</code> only if the required files in <code>processed_paths</code> are not present.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not implemented by a subclass.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.BaseDataset.shuffle","title":"shuffle","text":"<pre><code>shuffle(\n    return_perm: bool = False,\n) -&gt; \"BaseDataset | Tuple[BaseDataset, Tensor]\"\n</code></pre> <p>Return a shuffled subset view of the dataset.</p> <p>This method generates a random permutation of the current dataset view and returns a subset view with that ordering.</p> <p>Parameters:</p> Name Type Description Default <code>return_perm</code> <code>bool</code> <p>If True, also return the permutation tensor.</p> <code>False</code> <p>Returns:</p> Type Description <code>'BaseDataset | Tuple[BaseDataset, Tensor]'</code> <p>BaseDataset | tuple[BaseDataset, torch.Tensor]: If <code>return_perm</code> is False, returns the shuffled dataset view. If True, returns <code>(dataset, perm)</code> where <code>perm</code> is a 1D long tensor of indices into the current view.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.CoralDataset","title":"CoralDataset","text":"<pre><code>CoralDataset(\n    root: str,\n    fixed_transform: Callable[[FlooderData], FlooderData]\n    | None = None,\n    transform: Callable[[FlooderData], FlooderData]\n    | None = None,\n)\n</code></pre> <p>               Bases: <code>FlooderDataset</code></p> <p>Coral point-cloud dataset used in the Flooder paper.</p> <p>This dataset consists of 81 point clouds, each comprising 1 million points uniformly sampled from surface meshes of coral specimens provided by the Smithsonian 3D Digitization program (https://3d.si.edu/corals). Labels correspond to the coral's genus, with 31 Acroporidae samples (label 0) and 52 Poritidae samples (label 1).</p> <p>The dataset is distributed as a compressed archive (<code>corals.tar.zst</code>) hosted on Google Drive. The archive is downloaded, validated via SHA256, extracted into <code>raw_dir/&lt;folder_name&gt;/</code>, and processed into per-sample <code>.pt</code> files stored in <code>processed_dir</code>.</p> <p>Each raw sample is stored as a <code>.npy</code> array that is loaded and normalized by dividing by 32767, and cast to <code>float32</code>. Labels are read from the dataset metadata (<code>meta.yaml</code>) under <code>ydata['data'][&lt;filename&gt;]['label']</code>.</p> The processed sample is represented as <ul> <li><code>x</code>: <code>torch.FloatTensor</code> containing the point cloud</li> <li><code>y</code>: integer class label</li> <li><code>name</code>: sample identifier derived from the file stem</li> </ul> <p>Directory structure (expected after extraction):     raw_dir/corals/         meta.yaml         splits.yaml         *.npy</p> See Also <p>FlooderDataset: Implements the download/process/load lifecycle.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.CoralDataset.checksum","title":"checksum  <code>property</code>","text":"<pre><code>checksum: str\n</code></pre> <p>Expected SHA256 checksum of the downloaded archive.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Lowercase hex-encoded SHA256 digest for <code>corals.tar.zst</code>.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.CoralDataset.file_id","title":"file_id  <code>property</code>","text":"<pre><code>file_id: str\n</code></pre> <p>Google Drive file id for the dataset archive.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Google Drive file id used to construct the download URL.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.CoralDataset.raw_file_names","title":"raw_file_names  <code>property</code>","text":"<pre><code>raw_file_names: list[str]\n</code></pre> <p>Name of the extracted raw folder under <code>raw_dir</code>.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>list[str]</code> <p>Folder name containing raw dataset files after extraction.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.CoralDataset.process_file","title":"process_file","text":"<pre><code>process_file(file: Path, ydata: dict) -&gt; None\n</code></pre> <p>Convert a raw <code>.npy</code> file into a <code>FlooderData</code> example.</p> <p>Loads the point cloud from <code>file</code> using <code>numpy.load</code>, normalizes values by dividing by <code>32767</code>, casts to <code>float32</code>, and converts to a PyTorch tensor. The class label is read from metadata.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Path</code> <p>Path to the raw <code>.npy</code> file to process.</p> required <code>ydata</code> <code>dict</code> <p>Parsed YAML metadata from <code>meta.yaml</code>. Must contain an entry <code>ydata['data'][file.name]['label']</code>.</p> required <p>Returns:</p> Name Type Description <code>FlooderData</code> <code>None</code> <p>Processed example with fields <code>(x, y, name)</code>.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the expected label entry is missing from <code>ydata</code>.</p> <code>OSError</code> <p>If the <code>.npy</code> file cannot be read.</p> <code>ValueError</code> <p>If the <code>.npy</code> content cannot be converted to <code>float32</code>.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.FlooderDataset","title":"FlooderDataset","text":"<pre><code>FlooderDataset(\n    root: str,\n    fixed_transform: Callable[[FlooderData], FlooderData]\n    | None = None,\n    transform: Callable[[FlooderData], FlooderData]\n    | None = None,\n)\n</code></pre> <p>               Bases: <code>BaseDataset</code></p> <p>Base class for Flooder paper datasets distributed as compressed archives.</p> <p>This dataset class implements a standard pipeline:</p> <p>1) Download a <code>.tar.zst</code> archive from Google Drive (via <code>gdown</code>),      identified by <code>file_id</code>, and validate it with a SHA256 <code>checksum</code>.   2) Decompress and extract the archive into <code>raw_dir/&lt;folder_name&gt;/</code>.   3) Read dataset metadata (<code>meta.yaml</code>) and split definitions (<code>splits.yaml</code>)      from the extracted raw folder.   4) Convert each raw <code>.npy</code> file into a <code>FlooderData</code>-like object via      <code>process_file(...)</code>, and store it as a <code>.pt</code> file in <code>processed_dir</code>.   5) Load all <code>.pt</code> files into memory (<code>self.data</code>) in <code>_load()</code>, optionally      applying <code>fixed_transform</code> once per sample at load time.</p> Subclasses are expected to define <ul> <li><code>file_id</code>: Google Drive file id</li> <li><code>checksum</code>: expected SHA256 checksum of the downloaded archive</li> <li><code>folder_name</code>: name of the extracted folder under <code>raw_dir</code></li> <li><code>raw_file_names</code>: name(s) of the downloaded raw archive file(s)</li> <li><code>process_file(...)</code>: conversion logic from a <code>.npy</code> file and metadata</li> </ul> <p>Attributes:</p> Name Type Description <code>data</code> <code>list[FlooderData]</code> <p>In-memory list of processed examples loaded from <code>.pt</code> files. Populated by <code>_load()</code>.</p> <code>splits</code> <code>dict</code> <p>Split definitions loaded from <code>processed_dir/splits.yaml</code>. The structure depends on the dataset, but is typically a mapping from split identifier (e.g., fold index) to dicts containing keys like <code>'trn'</code>, <code>'val'</code>, <code>'tst'</code> with integer indices.</p> <code>classes</code> <code>list[int]</code> <p>Sorted list of unique class labels observed across the dataset (computed after loading).</p> <code>num_classes</code> <code>int</code> <p>Number of unique classes.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.FlooderDataset.checksum","title":"checksum  <code>property</code>","text":"<pre><code>checksum: str\n</code></pre> <p>Expected SHA256 checksum of the downloaded archive.</p> <p>The checksum is used by <code>validate(...)</code> after download. If the computed SHA256 does not match, a warning is emitted.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Lowercase hex-encoded SHA256 digest of the expected archive.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not implemented by a subclass.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.FlooderDataset.file_id","title":"file_id  <code>property</code>","text":"<pre><code>file_id: str\n</code></pre> <p>Google Drive file id for the dataset archive.</p> <p>Subclasses must provide the id used to construct the download URL: <code>https://drive.google.com/uc?id=&lt;file_id&gt;</code>.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The Google Drive file id.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not implemented by a subclass.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.FlooderDataset.folder_name","title":"folder_name  <code>property</code>","text":"<pre><code>folder_name: str\n</code></pre> <p>Name of the extracted folder under <code>raw_dir</code>.</p> <p>After extraction, the expected raw folder is <code>raw_dir/&lt;folder_name&gt;/</code> containing <code>meta.yaml</code>, <code>splits.yaml</code>, and the <code>.npy</code> files.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Folder name within <code>raw_dir</code>.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not implemented by a subclass.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.FlooderDataset.processed_file_names","title":"processed_file_names  <code>property</code>","text":"<pre><code>processed_file_names: list[str]\n</code></pre> <p>Processed-file sentinel list for determining whether processing is done.</p> The default convention for Flooder datasets is <ul> <li><code>_done</code>: an empty file indicating processing completion</li> <li><code>splits.yaml</code>: split definitions copied to the processed directory</li> </ul> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of required processed file names.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.FlooderDataset.download","title":"download","text":"<pre><code>download() -&gt; None\n</code></pre> <p>Download the dataset archive from Google Drive into <code>raw_dir</code>.</p> <p>Constructs a Google Drive download URL from <code>file_id</code> and downloads into <code>raw_dir/&lt;raw_file_names[0]&gt;</code> using <code>gdown</code>. After downloading, calls <code>validate(...)</code> to check integrity.</p> <p>Raises:</p> Type Description <code>IndexError</code> <p>If <code>raw_file_names</code> is empty.</p> <code>OSError</code> <p>If the destination file cannot be written.</p> <code>Exception</code> <p>Propagates errors from <code>gdown.download(...)</code>.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.FlooderDataset.get","title":"get","text":"<pre><code>get(idx: int) -&gt; FlooderData\n</code></pre> <p>Return the in-memory data object at the given global index.</p> <p>This implementation assumes <code>_load()</code> has populated <code>self.data</code> with objects saved in <code>processed_dir</code> as <code>.pt</code> files.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Global index into <code>self.data</code>.</p> required <p>Returns:</p> Name Type Description <code>FlooderData</code> <code>FlooderData</code> <p>The data item at <code>idx</code>.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.FlooderDataset.get_split_indices","title":"get_split_indices","text":"<pre><code>get_split_indices(splits_data) -&gt; dict\n</code></pre> <p>Extract split indices from raw <code>splits.yaml</code> content.</p> <p>The default behavior expects the raw <code>splits.yaml</code> to contain a top-level key <code>\"splits\"</code> holding the split definitions.</p> <p>Subclasses may override this method if their <code>splits.yaml</code> uses a different schema.</p> <p>Parameters:</p> Name Type Description Default <code>splits_data</code> <code>dict</code> <p>Parsed YAML content from <code>splits.yaml</code>.</p> required <p>Returns:</p> Name Type Description <code>Any</code> <code>dict</code> <p>The split indices structure to be saved into</p> <code>dict</code> <p><code>processed_dir/splits.yaml</code>. Typically a dict mapping fold id to split</p> <code>dict</code> <p>dicts (<code>'trn'</code>, <code>'val'</code>, <code>'tst'</code>), but may vary by dataset.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.FlooderDataset.len","title":"len","text":"<pre><code>len() -&gt; int\n</code></pre> <p>Return the number of examples in the full dataset.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Total number of examples, equal to <code>len(self.data)</code> after <code>_load()</code>.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.FlooderDataset.process","title":"process","text":"<pre><code>process() -&gt; None\n</code></pre> <p>Process the extracted raw dataset into serialized <code>.pt</code> files.</p> <p>Processing performs the following steps:</p> <p>1) Ensure the archive has been extracted into <code>raw_dir/&lt;folder_name&gt;/</code>.   2) Load metadata from <code>meta.yaml</code> and split definitions from <code>splits.yaml</code>.   3) Save extracted split indices into <code>processed_dir/splits.yaml</code>.   4) Iterate over all <code>.npy</code> files in the extracted folder, sorted by name.   5) For each <code>.npy</code>, call <code>process_file(file, ydata)</code> and save the returned      object as <code>&lt;stem&gt;.pt</code> in <code>processed_dir</code>.   6) Create the <code>_done</code> sentinel file in <code>processed_dir</code>.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If required raw files (<code>meta.yaml</code>, <code>splits.yaml</code>, or <code>.npy</code> files) are missing.</p> <code>YAMLError</code> <p>If YAML parsing fails.</p> <code>OSError</code> <p>For I/O errors reading raw files or writing processed files.</p> <code>RuntimeError</code> <p>If <code>torch.save</code> fails for a produced object.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.FlooderDataset.process_file","title":"process_file","text":"<pre><code>process_file(file: Path, ydata: dict) -&gt; None\n</code></pre> <p>Convert a raw <code>.npy</code> file into a <code>FlooderData</code>-like object.</p> <p>Subclasses must implement the dataset-specific logic for reading <code>file</code> (typically via <code>numpy.load</code>) and for producing an instance of <code>FlooderData</code> (or a subclass like <code>FlooderRocksData</code>).</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Path</code> <p>Path to a <code>.npy</code> file inside the extracted raw folder.</p> required <code>ydata</code> <code>dict</code> <p>Metadata loaded from <code>meta.yaml</code>. The structure is dataset- dependent but typically contains labels and other targets keyed by file name.</p> required <p>Returns:</p> Name Type Description <code>FlooderData</code> <code>None</code> <p>Processed data object to be saved as a <code>.pt</code> file.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If not implemented by a subclass.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.FlooderDataset.unzip_file","title":"unzip_file","text":"<pre><code>unzip_file() -&gt; None\n</code></pre> <p>Decompress and extract the dataset archive into <code>raw_dir</code>.</p> <p>This method reads the first file in <code>raw_paths</code> as a <code>.tar.zst</code> archive, decompresses it using <code>zstandard</code>, and extracts it using <code>tarfile</code>.</p> Extraction behavior depends on Python version <ul> <li>Python &gt;= 3.12: uses <code>tar.extractall(..., filter='data')</code> to apply   tarfile's safety filter.</li> <li>Older versions: falls back to <code>tar.extractall(...)</code>.</li> </ul> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If <code>raw_paths[0]</code> does not exist.</p> <code>TarError</code> <p>If the archive is invalid or cannot be read.</p> <code>ZstdError</code> <p>If decompression fails.</p> <code>OSError</code> <p>For I/O errors during reading or extraction.</p> Security <p>Be careful extracting archives from untrusted sources. While Python 3.12's <code>filter='data'</code> mitigates some risks, older versions extract without filtering.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.FlooderDataset.validate","title":"validate","text":"<pre><code>validate(file_path: Path) -&gt; None\n</code></pre> <p>Validate a downloaded archive against the expected SHA256 checksum.</p> <p>Computes the SHA256 digest of <code>file_path</code> and compares it to <code>self.checksum</code>. If they do not match, emits a <code>UserWarning</code>.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>Path | str</code> <p>Path to the downloaded archive.</p> required <p>Warns:</p> Type Description <code>UserWarning</code> <p>If the computed checksum differs from the expected checksum.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If <code>file_path</code> does not exist.</p> <code>OSError</code> <p>For I/O errors reading the file.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.LargePointCloudDataset","title":"LargePointCloudDataset","text":"<pre><code>LargePointCloudDataset(\n    root: str,\n    fixed_transform: Callable[[FlooderData], FlooderData]\n    | None = None,\n    transform: Callable[[FlooderData], FlooderData]\n    | None = None,\n)\n</code></pre> <p>               Bases: <code>FlooderDataset</code></p> <p>LargePointCloudDataset dataset with large-scale point-clouds used in the Flooder paper.</p> <p>This dataset contains two point clouds with more than 10M points each, distributed as a compressed <code>.tar.zst</code> archive and hosted on Google Drive. The archive is downloaded, validated via SHA256, extracted into <code>raw_dir/&lt;folder_name&gt;/</code>, and processed into per-sample <code>.pt</code> files.</p> The processed sample are stored as a LargePointCloudData dataclass with the following attributes <ul> <li><code>x</code>: <code>torch.FloatTensor</code> of point coordinates</li> <li><code>name</code>: sample identifier</li> <li><code>description</code>: brief description of the point cloud</li> </ul> Expected extracted raw directory structure <p>raw/large/     meta.yaml     coral.pt     virus.pt</p> See Also <p>FlooderDataset: Implements the shared download, processing, and loading pipeline.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.LargePointCloudDataset.checksum","title":"checksum  <code>property</code>","text":"<pre><code>checksum: str\n</code></pre> <p>Expected SHA256 checksum of the downloaded archive.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Lowercase hex-encoded SHA256 digest for <code>large.tar.zst</code>.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.LargePointCloudDataset.file_id","title":"file_id  <code>property</code>","text":"<pre><code>file_id: str\n</code></pre> <p>Google Drive file id for the LargePointCloudDataset dataset archive.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Google Drive file id used to construct the download URL.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.LargePointCloudDataset.folder_name","title":"folder_name  <code>property</code>","text":"<pre><code>folder_name: str\n</code></pre> <p>Name of the extracted raw folder under <code>raw_dir</code>.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Folder name containing the extracted large dataset files.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.LargePointCloudDataset.raw_file_names","title":"raw_file_names  <code>property</code>","text":"<pre><code>raw_file_names: list[str]\n</code></pre> <p>Raw archive file name(s) expected in <code>raw_dir</code>.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List containing the dataset archive file name.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.LargePointCloudDataset.uncompressed_file_names","title":"uncompressed_file_names  <code>property</code>","text":"<pre><code>uncompressed_file_names: list[str]\n</code></pre> <p>Uncompressed file name(s) expected in <code>raw_dir</code>.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List containing the uncompressed file names.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.LargePointCloudDataset.get","title":"get","text":"<pre><code>get(idx) -&gt; LargePointCloudData\n</code></pre> <p>Return the data object at a given index (either 0 or 1).</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index to access.</p> required <p>Returns:</p> Name Type Description <code>LargePointCloudData</code> <code>LargePointCloudData</code> <p>The data object at the given index.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.LargePointCloudDataset.len","title":"len","text":"<pre><code>len() -&gt; int\n</code></pre> <p>Return the number of items in the full dataset.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.LargePointCloudDataset.process","title":"process","text":"<pre><code>process() -&gt; None\n</code></pre> <p>Extract the raw dataset.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.MCBDataset","title":"MCBDataset","text":"<pre><code>MCBDataset(\n    root: str,\n    fixed_transform: Callable[[FlooderData], FlooderData]\n    | None = None,\n    transform: Callable[[FlooderData], FlooderData]\n    | None = None,\n)\n</code></pre> <p>               Bases: <code>FlooderDataset</code></p> <p>MCB point-cloud dataset used in the Flooder paper.</p> <p>This dataset consists of 1745 point clouds, each comprising 1 million points uniformly sampled from surface meshes from a subset of the MCB dataset (A large-scale annotated mechanical components benchmark for classification and retrieval tasks with deep neural networks, ECCV, 2020) available at https://github.com/stnoah1/mcb.</p> <p>The dataset is distributed as a compressed <code>.tar.zst</code> archive hosted on Google Drive. The archive is downloaded, validated using a SHA256 checksum, extracted into <code>raw_dir/&lt;folder_name&gt;/</code>, and processed into per-sample <code>.pt</code> files stored in <code>processed_dir</code>.</p> <p>Each raw sample is stored as a <code>.npy</code> array containing quantized point coordinates. During processing, coordinates are normalized by dividing by <code>32767</code> and cast to <code>float32</code>.</p> The processed sample representation is <ul> <li><code>x</code>: <code>torch.FloatTensor</code> of normalized point coordinates</li> <li><code>y</code>: integer class label</li> <li><code>name</code>: sample identifier derived from the file stem</li> </ul> Expected extracted raw directory structure <p>raw_dir/mcb/     meta.yaml     splits.yaml     *.npy</p> See Also <p>FlooderDataset: Implements the shared download, processing, and loading pipeline.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.MCBDataset.checksum","title":"checksum  <code>property</code>","text":"<pre><code>checksum: str\n</code></pre> <p>Expected SHA256 checksum of the downloaded archive.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Lowercase hex-encoded SHA256 digest for <code>mcb.tar.zst</code>.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.MCBDataset.file_id","title":"file_id  <code>property</code>","text":"<pre><code>file_id: str\n</code></pre> <p>Google Drive file id for the MCB dataset archive.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Google Drive file id used to construct the download URL.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.MCBDataset.folder_name","title":"folder_name  <code>property</code>","text":"<pre><code>folder_name: str\n</code></pre> <p>Name of the extracted raw folder under <code>raw_dir</code>.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Folder name containing the extracted MCB dataset files.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.MCBDataset.raw_file_names","title":"raw_file_names  <code>property</code>","text":"<pre><code>raw_file_names: list[str]\n</code></pre> <p>Raw archive file name(s) expected in <code>raw_dir</code>.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List containing the dataset archive file name.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.MCBDataset.process_file","title":"process_file","text":"<pre><code>process_file(file: Path, ydata: dict) -&gt; FlooderData\n</code></pre> <p>Convert a raw <code>.npy</code> file into a <code>FlooderData</code> example.</p> <p>Loads the raw point cloud from <code>file</code>, normalizes coordinates by dividing by <code>32767</code>, casts to <code>float32</code>, and converts to a PyTorch tensor. The class label is read from the dataset metadata.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Path</code> <p>Path to the raw <code>.npy</code> file inside the extracted MCB dataset folder.</p> required <code>ydata</code> <code>dict</code> <p>Parsed YAML metadata from <code>meta.yaml</code>. Must contain an entry <code>ydata['data'][file.name]['label']</code>.</p> required <p>Returns:</p> Name Type Description <code>FlooderData</code> <code>FlooderData</code> <p>Processed example with fields <code>(x, y, name)</code>.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the label entry for <code>file.name</code> is missing in <code>ydata</code>.</p> <code>OSError</code> <p>If the <code>.npy</code> file cannot be read.</p> <code>ValueError</code> <p>If the loaded array cannot be converted to <code>float32</code>.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.ModelNet10Dataset","title":"ModelNet10Dataset","text":"<pre><code>ModelNet10Dataset(\n    root: str,\n    fixed_transform: Callable[[FlooderData], FlooderData]\n    | None = None,\n    transform: Callable[[FlooderData], FlooderData]\n    | None = None,\n)\n</code></pre> <p>               Bases: <code>FlooderDataset</code></p> <p>ModelNet10 point-cloud dataset (250k points) used in the Flooder paper.</p> <p>This dataset consists of 4899 point clouds, each comprising 250k points uniformly sampled from surface meshes from the ModelNet10 dataset (Wu et al., 3D ShapeNets: A Deep Representation for Volumetric Shapes, CVPR 2015) available at https://modelnet.cs.princeton.edu/.</p> <p>The dataset is distributed as a compressed <code>.tar.zst</code> archive hosted on Google Drive. The archive is downloaded, validated using a SHA256 checksum, extracted into <code>raw_dir/&lt;folder_name&gt;/</code>, and processed into per-sample <code>.pt</code> files stored in <code>processed_dir</code>.</p> <p>Each raw sample is stored as a <code>.npy</code> array containing quantized point coordinates. During processing, coordinates are normalized by dividing by <code>32767</code> and cast to <code>float32</code>.</p> The processed sample representation is <ul> <li><code>x</code>: <code>torch.FloatTensor</code> of normalized point coordinates</li> <li><code>y</code>: integer class label in <code>[0, 9]</code></li> <li><code>name</code>: sample identifier derived from the file stem</li> </ul> Expected extracted raw directory structure <p>raw_dir/modelnet10_250k/     meta.yaml     splits.yaml     *.npy</p> See Also <p>FlooderDataset: Implements the shared download, processing, and loading pipeline.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.RocksDataset","title":"RocksDataset","text":"<pre><code>RocksDataset(\n    root: str,\n    fixed_transform: Callable[[FlooderData], FlooderData]\n    | None = None,\n    transform: Callable[[FlooderData], FlooderData]\n    | None = None,\n)\n</code></pre> <p>               Bases: <code>FlooderDataset</code></p> <p>Rock voxel dataset converted to point clouds with geometric targets.</p> <p>This synthetic dataset consists of 1000 3D binary voxel grids representing rock samples from two classes. The voxel grids are produced by the PoreSpy library (https://porespy.org/) with classes corresponding to the generation method, fractal noise and blobs, each with 500 samples.</p> <p>The dataset is distributed as a compressed archive (<code>rocks.tar.zst</code>) hosted on Google Drive. During processing, each voxel grid is converted into a set of 3D points by extracting the coordinates of occupied voxels and adding small random jitter to break lattice the lattice structure.</p> <p>In addition to the class label, each sample includes continuous targets such as surface area and volume.</p> Processed sample representation <ul> <li><code>x</code>: <code>torch.FloatTensor</code> of shape <code>(N, 3)</code> containing point coordinates</li> <li><code>y</code>: integer class label</li> <li><code>surface</code>: float-valued surface area target</li> <li><code>volume</code>: float-valued volume target</li> <li><code>name</code>: sample identifier derived from the file stem</li> </ul> Expected extracted raw directory structure <p>raw_dir/rocks/     meta.yaml     splits.yaml     *.npy</p> See Also <p>FlooderDataset: Implements the shared download, processing, and loading pipeline.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.RocksDataset.process_file","title":"process_file","text":"<pre><code>process_file(file: Path, ydata: dict) -&gt; FlooderRocksData\n</code></pre> <p>Convert a raw voxel <code>.npy</code> file into a <code>FlooderRocksData</code> example.</p> Processing steps <p>1) Load the bit-packed voxel array from <code>file</code>. 2) Unpack bits into a boolean array of shape <code>(256, 256, 256)</code>. 3) Extract the indices of occupied voxels using <code>np.where</code>. 4) Convert voxel indices to float coordinates and add small random    jitter to avoid degenerate lattice structure. 5) Attach label and continuous targets from metadata.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Path</code> <p>Path to the raw <code>.npy</code> voxel file.</p> required <code>ydata</code> <code>dict</code> <p>Parsed YAML metadata from <code>meta.yaml</code>. Must contain entries for <code>label</code>, <code>target</code> (surface), and <code>volume</code> under <code>ydata['data'][file.name]</code>.</p> required <p>Returns:</p> Name Type Description <code>FlooderRocksData</code> <code>FlooderRocksData</code> <p>Processed example with fields</p> <code>FlooderRocksData</code> <p><code>(x, y, surface, volume, name)</code>.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If required metadata entries are missing.</p> <code>ValueError</code> <p>If the unpacked voxel array cannot be reshaped to <code>(256, 256, 256)</code>.</p> <code>OSError</code> <p>If the <code>.npy</code> file cannot be read.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.SwisscheeseDataset","title":"SwisscheeseDataset","text":"<pre><code>SwisscheeseDataset(\n    root: str,\n    ks: list[int] = [10, 20],\n    num_per_class: int = 500,\n    num_points: int = 1000000,\n    fixed_transform: Callable[[FlooderData], FlooderData]\n    | None = None,\n    transform: Callable[[FlooderData], FlooderData]\n    | None = None,\n)\n</code></pre> <p>               Bases: <code>FlooderDataset</code></p> <p>Synthetic \"Swiss cheese\" point-cloud dataset used in the Flooder paper.</p> <p>This dataset is generated procedurally (no download). Each sample consists of points uniformly sampled from a 3D axis-aligned box with multiple spherical voids removed (\"Swiss cheese\"). The number of voids defines the class label.</p> <p>Unlike the other <code>FlooderDataset</code> subclasses that download a compressed archive, this dataset overrides <code>process()</code> to generate samples and write them directly to <code>processed_dir</code> as <code>.pt</code> files. Split definitions are also generated and saved to <code>processed_dir/splits.yaml</code>.</p> Class semantics <ul> <li>Each class corresponds to a value <code>k</code> in <code>ks</code>, where <code>k</code> is the number   of spherical voids carved out of the sampling volume.</li> <li>Label <code>y</code> is the integer index into <code>ks</code> (i.e., <code>ki</code> from enumeration).</li> </ul> Generated file naming <p>Each sample is saved under a short SHA256-derived identifier computed from the generated point array bytes. This provides deterministic naming for a fixed RNG seed and generation implementation, but note that changes to sampling code, dtype, or ordering can change the resulting hash.</p> Splits <p><code>process()</code> generates 10 random splits (keys <code>0..9</code>), each containing <code>trn</code>, <code>val</code>, and <code>tst</code> partitions with proportions 72% / 8% / 20%, respectively, over the full dataset indices.</p> Notes <ul> <li>Because generation is performed inside <code>process()</code>, instantiation may be   compute- and storage-intensive, depending on <code>num_points</code> and dataset size.</li> <li>This class sets a fixed RNG seed (<code>np.random.RandomState(42)</code>) for   split generation. The point generation itself depends on the behavior of   <code>generate_swiss_cheese_points</code> (and any randomness inside it).</li> </ul> <p>Initialize the Swiss cheese dataset generator.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>Root directory where the dataset is stored.</p> required <code>ks</code> <code>list[int]</code> <p>List of void counts, one per class. Each <code>k</code> produces a distinct class corresponding to point clouds with <code>k</code> voids.</p> <code>[10, 20]</code> <code>num_per_class</code> <code>int</code> <p>Number of samples generated for each class. Total dataset size is <code>len(ks) * num_per_class</code>.</p> <code>500</code> <code>num_points</code> <code>int</code> <p>Number of points generated per sample point cloud.</p> <code>1000000</code> <code>fixed_transform</code> <code>Callable[[FlooderData], FlooderData] | None</code> <p>Optional transform applied once per example during <code>_load()</code>.</p> <code>None</code> <code>transform</code> <code>Callable[[FlooderData], FlooderData] | None</code> <p>Optional transform applied on-the-fly in <code>__getitem__</code>.</p> <code>None</code> Notes <ul> <li>Split generation uses a fixed seed (<code>42</code>) via <code>np.random.RandomState</code>.</li> <li>Generation and serialization are performed during <code>process()</code>, which is   invoked during <code>FlooderDataset</code> construction if processed artifacts   are missing.</li> </ul>"},{"location":"reference/datasets/#flooder.datasets.datasets.SwisscheeseDataset.folder_name","title":"folder_name  <code>property</code>","text":"<pre><code>folder_name: str\n</code></pre> <p>Name of the raw folder under <code>raw_dir</code>.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Folder name. Included for API compatibility; this dataset does not</p> <code>str</code> <p>use extracted raw archives.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.SwisscheeseDataset.raw_file_names","title":"raw_file_names  <code>property</code>","text":"<pre><code>raw_file_names: list[str]\n</code></pre> <p>Raw-file requirements for download skipping.</p> <p>This dataset is generated locally and does not require downloaded raw files, so this returns an empty list.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: Empty list.</p>"},{"location":"reference/datasets/#flooder.datasets.datasets.SwisscheeseDataset.process","title":"process","text":"<pre><code>process() -&gt; None\n</code></pre> <p>Generate synthetic samples and write processed artifacts to disk.</p> This method generates <ul> <li><code>processed_dir/splits.yaml</code>: A dict of 10 random splits with keys <code>0..9</code>.</li> <li>One <code>.pt</code> file per generated sample containing a <code>FlooderData</code> object.</li> <li><code>processed_dir/_done</code>: A sentinel file indicating processing completion.</li> </ul> Sample generation details <ul> <li>Points are generated inside an axis-aligned box with corners   <code>rect_min = [0,0,0]</code> and <code>rect_max = [5,5,5]</code>.</li> <li>For each class <code>k</code> in <code>ks</code>, the generator creates <code>num_per_class</code>   samples using <code>generate_swiss_cheese_points(num_points, ..., k, ...)</code>.</li> <li>Each sample is labeled with <code>y = ki</code> where <code>ki</code> is the index of <code>k</code> in <code>ks</code>.</li> </ul> <p>Raises:</p> Type Description <code>OSError</code> <p>If the processed directory cannot be written.</p> <code>RuntimeError</code> <p>If <code>torch.save</code> fails.</p> <code>Exception</code> <p>Propagates exceptions from <code>generate_swiss_cheese_points</code>.</p>"},{"location":"reference/io/","title":"Utils","text":""},{"location":"reference/io/#flooder.io","title":"flooder.io","text":"<p>IO functionality (for consistent saving).</p> <p>Copyright (c) 2025 Paolo Pellizzoni, Florian Graf, Martin Uray, Stefan Huber and Roland Kwitt SPDX-License-Identifier: MIT</p>"},{"location":"reference/io/#flooder.io.save_to_disk","title":"save_to_disk","text":"<pre><code>save_to_disk(\n    obj: Any,\n    path: Union[str, Path],\n    metadata: bool = True,\n    overwrite: bool = False,\n) -&gt; None\n</code></pre> <p>Save an object to disk using <code>torch.save</code>.</p> <p>This function saves any Python object to a specified path. If the object is a dictionary and <code>metadata=True</code>, a <code>_meta</code> entry is added to the copy with timestamp and key information. If the file already exists and <code>overwrite=False</code>, an error is raised.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>The Python object to save (e.g., a tensor, dictionary, or model).</p> required <code>path</code> <code>Union[str, Path]</code> <p>The file path where the object will be saved.</p> required <code>metadata</code> <code>bool</code> <p>Whether to include metadata in the saved file (only applicable if <code>obj</code> is a dictionary). Defaults to True.</p> <code>True</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the file if it already exists. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>FileExistsError</code> <p>If the file already exists and <code>overwrite</code> is False.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p>"},{"location":"reference/synthetic_data_generators/","title":"Synthetic data generators","text":""},{"location":"reference/synthetic_data_generators/#flooder.synthetic_data_generators","title":"flooder.synthetic_data_generators","text":"<p>Implementation of synthetic data generators.</p> <p>Copyright (c) 2025 Paolo Pellizzoni, Florian Graf, Martin Uray, Stefan Huber and Roland Kwitt SPDX-License-Identifier: MIT</p>"},{"location":"reference/synthetic_data_generators/#flooder.synthetic_data_generators.generate_annulus_points_2d","title":"generate_annulus_points_2d","text":"<pre><code>generate_annulus_points_2d(\n    n: int = 1000,\n    center: tensor = torch.tensor([0.0, 0.0]),\n    radius: float = 1.0,\n    width: float = 0.2,\n    seed: int = None,\n) -&gt; torch.tensor\n</code></pre> <p>Generate 2D points uniformly distributed in the region between two concentric circles.</p> <p>In particulr, points are sampled uniformly within a ring defined by an outer <code>radius</code> and an inner radius of <code>radius - width</code>, centered at a specified 2D location.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of points to generate. Defaults to 1000.</p> <code>1000</code> <code>center</code> <code>Tensor</code> <p>Center of the annulus as a tensor of shape (2,). Defaults to [0.0, 0.0].</p> <code>tensor([0.0, 0.0])</code> <code>radius</code> <code>float</code> <p>Outer radius of the annulus. Must be positive. Defaults to 1.0.</p> <code>1.0</code> <code>width</code> <code>float</code> <p>Thickness of the annulus. Must be positive and less than <code>radius</code>. Defaults to 0.2.</p> <code>0.2</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. If None, randomness is not seeded.</p> <code>None</code> <p>Returns:</p> Type Description <code>tensor</code> <p>torch.Tensor: A tensor of shape (N, 2) containing the sampled 2D points.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; center = torch.tensor([0.0, 0.0])\n&gt;&gt;&gt; points = generate_annulus_points_2d(n=500, center=center, radius=1.0, width=0.3, seed=42)\n&gt;&gt;&gt; points.shape\ntorch.Size([500, 2])\n</code></pre>"},{"location":"reference/synthetic_data_generators/#flooder.synthetic_data_generators.generate_figure_eight_points_2d","title":"generate_figure_eight_points_2d","text":"<pre><code>generate_figure_eight_points_2d(\n    n: int = 1000,\n    r_bounds: Tuple[float, float] = (0.2, 0.3),\n    centers: Tuple[\n        Tuple[float, float], Tuple[float, float]\n    ] = ((0.3, 0.5), (0.7, 0.5)),\n    noise_std: float = 0.0,\n    noise_kind: Literal[\"gaussian\", \"uniform\"] = \"gaussian\",\n    seed: int = None,\n) -&gt; torch.tensor\n</code></pre> <p>Generate 2D points uniformly sampled in a figure-eight shape, with optional noise.</p> <p>This function samples <code>n_samples</code> points distributed across two circular lobes (forming a figure-eight shape) centered at specified coordinates. Optionally, isotropic Gaussian or uniform noise can be added to the coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of 2D points to generate. Defaults to 1000.</p> <code>1000</code> <code>r_bounds</code> <code>Tuple[float, float]</code> <p>Tuple specifying the minimum and maximum radius for sampling within each lobe. Defaults to (0.2, 0.3).</p> <code>(0.2, 0.3)</code> <code>centers</code> <code>Tuple[Tuple[float, float], Tuple[float, float]]</code> <p>Coordinates of the centers of the two lobes. Defaults to ((0.3, 0.5), (0.7, 0.5)).</p> <code>((0.3, 0.5), (0.7, 0.5))</code> <code>noise_std</code> <code>float</code> <p>Standard deviation (for Gaussian) or half-width (for uniform) of noise to add to each point. Defaults to 0.0 (no noise).</p> <code>0.0</code> <code>noise_kind</code> <code>Literal['gaussian', 'uniform']</code> <p>Type of noise distribution to use if <code>noise_std &gt; 0</code>. Defaults to \"gaussian\".</p> <code>'gaussian'</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. If None, randomness is not seeded.</p> <code>None</code> <p>Returns:</p> Type Description <code>tensor</code> <p>torch.Tensor: A tensor of shape (n_samples, 2) containing the sampled 2D points.</p>"},{"location":"reference/synthetic_data_generators/#flooder.synthetic_data_generators.generate_noisy_torus_points_3d","title":"generate_noisy_torus_points_3d","text":"<pre><code>generate_noisy_torus_points_3d(\n    n=1000,\n    R: float = 3.0,\n    r: float = 1.0,\n    noise_std: float = 0.02,\n    seed: int = None,\n) -&gt; torch.tensor\n</code></pre> <p>Generate 3D points on a torus with added Gaussian noise.</p> <p>Points are uniformly sampled on the surface of a torus defined by a major radius <code>R</code> and a minor radius <code>r</code>. Gaussian noise with standard deviation <code>noise_std</code> is added to each point independently in x, y, and z dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of points to generate. Defaults to 1000. R (float, optional): Major radius of the torus (distance from the center of the tube to the center of the torus). Must be positive. Defaults to 3.0.</p> <code>1000</code> <code>r</code> <code>float</code> <p>Minor radius of the torus (radius of the tube). Must be positive. Defaults to 1.0.</p> <code>1.0</code> <code>noise_std</code> <code>float</code> <p>Standard deviation of the Gaussian noise added to the points. Defaults to 0.02.</p> <code>0.02</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. If None, randomness is not seeded.</p> <code>None</code> <p>Returns:</p> Type Description <code>tensor</code> <p>torch.Tensor: A tensor of shape (num_points, 3) containing the generated noisy 3D points.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; points = generate_noisy_torus_points_3d(\n        n=500, R=3.0, r=1.0, noise_std=0.05, seed=123)\n&gt;&gt;&gt; points.shape\ntorch.Size([500, 3])\n</code></pre>"},{"location":"reference/synthetic_data_generators/#flooder.synthetic_data_generators.generate_swiss_cheese_points","title":"generate_swiss_cheese_points","text":"<pre><code>generate_swiss_cheese_points(\n    n: int = 1000,\n    rect_min: tuple = (0.0, 0.0, 0.0),\n    rect_max: tuple = (1.0, 1.0, 1.0),\n    k: int = 6,\n    void_radius_range: tuple = (0.1, 0.2),\n    seed: int = None,\n    *,\n    device=\"cpu\",\n    batch_factor=4,\n) -&gt; Tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n</code></pre> <p>Generate points in a high-dimensional rectangular region with randomly placed spherical voids, forming a \"Swiss cheese\" structure.</p> <p>Points are sampled uniformly within the bounding box defined by <code>rect_min</code> and <code>rect_max</code>, excluding k randomly positioned spherical voids with radii sampled from <code>void_radius_range</code>.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of points to generate. Defaults to 1000.</p> <code>1000</code> <code>rect_min</code> <code>tuple</code> <p>Minimum coordinates of the rectangular region. Defaults to a tuple of three zeros.</p> <code>(0.0, 0.0, 0.0)</code> <code>rect_max</code> <code>tuple</code> <p>Maximum coordinates of the rectangular region. Defaults to a tuple of three ones.</p> <code>(1.0, 1.0, 1.0)</code> <code>k</code> <code>int</code> <p>Number of spherical voids to generate. Defaults to 6.</p> <code>6</code> <code>void_radius_range</code> <code>Tuple[float, float]</code> <p>Range <code>(min_radius, max_radius)</code> for the void radii. Defaults to (0.1, 0.2).</p> <code>(0.1, 0.2)</code> <code>seed</code> <code>int</code> <p>Random seed for reproducibility. If None, randomness is not seeded.</p> <code>None</code> <code>device</code> <code>device</code> <p>Device to perform computations on. Defaults to 'cpu'.</p> <code>'cpu'</code> <code>batch_factor</code> <code>int</code> <p>How many candidates to shoot each round. Defaults to 4.</p> <code>4</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Tensor]</code> <p>Tuple[torch.Tensor, torch.Tensor]: A tuple containing: - <code>points</code> (torch.Tensor): Tensor of shape (N, dim) with generated sample points. - <code>void_radii</code> (torch.Tensor): Tensor of shape (k,) with the radii of the voids.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; rect_min = (0.0, 0.0, 0.0)\n&gt;&gt;&gt; rect_max = (1.0, 1.0, 1.0)\n&gt;&gt;&gt; void_radius_range = (0.1, 0.2)\n&gt;&gt;&gt; k = 5\n&gt;&gt;&gt; points, _ = generate_swiss_cheese_points(\n...     1000000, rect_min, rect_max, k, void_radius_range\n... )\n&gt;&gt;&gt; points.shape\ntorch.Size([1000000, 3])\n</code></pre>"}]}